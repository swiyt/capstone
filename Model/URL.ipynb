{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v3wJLq07xq3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = pd.read_csv('data_url.csv')\n",
        "url_df =  pd.DataFrame(url)\n",
        "url_df = np.array(url_df)\n",
        "url_df[0:5]\n",
        "#Set the array as the first data as url and the second as malware or benign"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeZmaMlDx9pD",
        "outputId": "5e372458-be50-4d96-e076-f91c4d40e960"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['diaryofagameaddict.com', 'bad'],\n",
              "       ['espdesign.com.au', 'bad'],\n",
              "       ['iamagameaddict.com', 'bad'],\n",
              "       ['kalantzis.net', 'bad'],\n",
              "       ['slightlyoffcenter.net', 'bad']], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "urls = []\n",
        "for d in url_df:\n",
        "    urls.append(d[0])\n",
        "    y.append(d[1])"
      ],
      "metadata": {
        "id": "LqdgM2EWyIcF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitization(url):\n",
        "  token = []\n",
        "  dot_token_slash = []\n",
        "  raw_url = str(url).split('/')\n",
        "\n",
        "  for i in raw_url:\n",
        "    raw1 = str(i).split('-')\n",
        "    slash_token = []\n",
        "\n",
        "    for j in range(0, len(raw1)):\n",
        "      raw2 = raw1[j].split('.')\n",
        "      slash_token = slash_token + raw2\n",
        "\n",
        "    dot_token_slash = dot_token_slash + raw1 + slash_token\n",
        "    token = list(set(dot_token_slash))\n",
        "    return token\n",
        "\n"
      ],
      "metadata": {
        "id": "KuQBKqxJ2cYB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(tokenizer=sanitization)"
      ],
      "metadata": {
        "id": "XglmFuHTh9Sh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = vectorizer.fit_transform(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_zuXk_biJF0",
        "outputId": "67bde74e-026a-41af-cc79-3794a702a26f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf3Oo7ZMiUtB",
        "outputId": "9fdb2f31-69b9-4742-96fe-06cfd0b5748f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<420464x277694 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1429458 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AO6_BNyXigov"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "lgr.fit(x_train, y_train)\n",
        "score = lgr.score(x_test, y_test)\n",
        "print(\"score: {0:.2f} %\".format(100 * score))\n",
        "vectorizer_save = vectorizer\n"
      ],
      "metadata": {
        "id": "iVTgEI6Lv9B0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bdcd08-81a3-478c-fb44-dff8fb1cf932"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score: 93.64 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file = \"pickel_model.pkl\"\n",
        "with open(file, 'wb') as f:\n",
        "    pickle.dump(lgr, f)\n",
        "f.close()\n",
        "\n",
        "file2 = \"pickle_vector.pkl\"\n",
        "with open(file2, 'wb') as f2:\n",
        "    pickle.dump(vectorizer_save, f2)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "4WkcrhYewHgh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xzxxa8en3ppn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}